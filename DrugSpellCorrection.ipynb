{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT3zfj8N8fRFTXIeZKUD2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SivAsanjai/SivAsanjai/blob/main/DrugSpellCorrection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yENXd-oiCAPy",
        "outputId": "1409d1dc-4480-4cac-ba87-e1dc390f4515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Installing collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk \n",
        "from nltk import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "drug_dict = {}\n",
        "with open('/content/drug_info_clean_1k.csv' , 'r') as file:\n",
        "  csv_reader = csv.reader(file)\n",
        "  header = next(csv_reader)\n",
        "  for row in csv_reader:\n",
        "    drug_name = row[header.index('description')].split(' ', 1)[0]\n",
        "    drug_dict[drug_name.lower()] = row\n",
        "n= 3\n",
        "corpus = ' '.join(list(drug_dict.keys()))\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "model  = defaultdict(Counter)\n",
        "for ngram in ngrams_list:\n",
        "  ngram_till_n_1 = tuple(ngram[:-1])\n",
        "  last_word = ngram[-1]\n",
        "  model[ngram_till_n_1][last_word] += 1\n",
        "\n",
        "spell = SpellChecker(language=None, case_sensitive=True)\n",
        "spell.word_frequency.load_words(drug_dict.keys())\n",
        "\n",
        "def correct_spelling(word):\n",
        "  return spell.correction(word)\n",
        "\n",
        "print(correct_spelling('Yevozan'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1-OIH2_Sr5s",
        "outputId": "fb92f7f0-f536-4b1a-e14d-448f9e96ef4a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yevozen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Parse the CSV\n",
        "drug_dict = {}\n",
        "with open ('/content/drug_info_clean_1k.csv', 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    header = next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        drug_name = row[header.index('description')].split(' ', 1)[0]\n",
        "        drug_dict[drug_name.lower()] = row\n",
        "n= 3\n",
        "corpus = ' '.join(list(drug_dict.keys()))\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "model  = defaultdict(Counter)\n",
        "for ngram in ngrams_list:\n",
        "  ngram_till_n_1 = tuple(ngram[:-1])\n",
        "  last_word = ngram[-1]\n",
        "  model[ngram_till_n_1][last_word] += 1\n",
        "\n",
        "spell = SpellChecker(language=None, case_sensitive=True)\n",
        "spell.word_frequency.load_words(drug_dict.keys())\n",
        "\n",
        "def correct_spelling(word):\n",
        "    blob = TextBlob(word)\n",
        "    correct_word = blob.correct()\n",
        "    return correct_word if correct_word in drug_dict else word \n",
        "\n",
        "# Test spelling correction\n",
        "print(correct_spelling('Yevizan'))\n"
      ],
      "metadata": {
        "id": "C_pKNXRWVZ-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237e4b83-f5de-43e4-8ee2-bd440575f501"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yevizan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(word, n = 3):\n",
        "  return set(ngrams(word, n))\n",
        "def jaccard_similarity(set1, set2):\n",
        "  intersection = set1.intersection(set2)\n",
        "  union = set1.union(set2)\n",
        "  return len(intersection)/len(union)\n",
        "def correct_spelling(word):\n",
        "  word_ngrams = get_ngrams(word)\n",
        "  similarities = [(drug, jaccard_similarity(word_ngrams,get_ngrams(drug))) for drug in drug_dict.keys()]\n",
        "  most_similar = max(similarities, key = lambda x: x[1])\n",
        "  return most_similar[0] if most_similar[0] in drug_dict.keys() else None\n"
      ],
      "metadata": {
        "id": "t4gYytDenBtF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct_spelling('Yonikest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWMOEr1foO10",
        "outputId": "59257b98-c393-46d0-a410-3f2fba629ff6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yonikast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "bjnBtlbktsoS",
        "outputId": "c2c873fb-56aa-4ae9-95d9-4bded33b28a7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-f6073940e26d>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sequences = array(sequences)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-f6073940e26d>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWmpcsTVQ_rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import array \n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "Wq4Tl25IxwmZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_names = list(drug_dict.keys())\n",
        "chars = sorted(list(set(''.join(drug_names))))\n",
        "mapping = dict((c, i) for i,c in enumerate(chars))\n",
        "sequences = []\n",
        "for name in drug_names:\n",
        "  encoded_seq = [mapping[char] for char in name]\n",
        "  for i in range(1, len(encoded_seq)):\n",
        "    sequences.append(encoded_seq[:i+1])\n",
        "\n",
        "sequences = pad_sequences(sequences)\n",
        "x,y = sequences[:,:-1], sequences[:, -1]\n",
        "x = x / float(len(mapping))\n",
        "y = to_categorical(y, num_classes = len(mapping))\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(75, input_shape = (x.shape[1],1)))\n",
        "model.add(Dense(len(mapping),activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1],1)\n",
        "\n",
        "model.fit(x_train, y_train, epochs = 100, verbose = 2, validation_data = (x_test, y_test))\n",
        "\n",
        "model.save('drug_lstm1.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml_nBVLmF_fc",
        "outputId": "d62660c8-fa88-4ec1-88ae-5a3643b9db4b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "97/97 - 4s - loss: 3.0889 - accuracy: 0.1198 - val_loss: 2.9244 - val_accuracy: 0.1358 - 4s/epoch - 45ms/step\n",
            "Epoch 2/100\n",
            "97/97 - 1s - loss: 2.9637 - accuracy: 0.1240 - val_loss: 2.9141 - val_accuracy: 0.1268 - 930ms/epoch - 10ms/step\n",
            "Epoch 3/100\n",
            "97/97 - 1s - loss: 2.9499 - accuracy: 0.1250 - val_loss: 2.8966 - val_accuracy: 0.1255 - 971ms/epoch - 10ms/step\n",
            "Epoch 4/100\n",
            "97/97 - 1s - loss: 2.9249 - accuracy: 0.1292 - val_loss: 2.8838 - val_accuracy: 0.1320 - 944ms/epoch - 10ms/step\n",
            "Epoch 5/100\n",
            "97/97 - 1s - loss: 2.9153 - accuracy: 0.1279 - val_loss: 2.8813 - val_accuracy: 0.1384 - 960ms/epoch - 10ms/step\n",
            "Epoch 6/100\n",
            "97/97 - 1s - loss: 2.9055 - accuracy: 0.1253 - val_loss: 2.8537 - val_accuracy: 0.1345 - 935ms/epoch - 10ms/step\n",
            "Epoch 7/100\n",
            "97/97 - 1s - loss: 2.8890 - accuracy: 0.1321 - val_loss: 2.8547 - val_accuracy: 0.1371 - 1s/epoch - 10ms/step\n",
            "Epoch 8/100\n",
            "97/97 - 1s - loss: 2.8708 - accuracy: 0.1354 - val_loss: 2.8381 - val_accuracy: 0.1358 - 1s/epoch - 12ms/step\n",
            "Epoch 9/100\n",
            "97/97 - 2s - loss: 2.8514 - accuracy: 0.1376 - val_loss: 2.7968 - val_accuracy: 0.1410 - 2s/epoch - 16ms/step\n",
            "Epoch 10/100\n",
            "97/97 - 1s - loss: 2.8342 - accuracy: 0.1386 - val_loss: 2.7818 - val_accuracy: 0.1410 - 1s/epoch - 12ms/step\n",
            "Epoch 11/100\n",
            "97/97 - 1s - loss: 2.8213 - accuracy: 0.1460 - val_loss: 2.7710 - val_accuracy: 0.1423 - 1s/epoch - 12ms/step\n",
            "Epoch 12/100\n",
            "97/97 - 1s - loss: 2.8025 - accuracy: 0.1399 - val_loss: 2.7654 - val_accuracy: 0.1475 - 1s/epoch - 12ms/step\n",
            "Epoch 13/100\n",
            "97/97 - 1s - loss: 2.7971 - accuracy: 0.1418 - val_loss: 2.7623 - val_accuracy: 0.1449 - 973ms/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "97/97 - 1s - loss: 2.7853 - accuracy: 0.1454 - val_loss: 2.7448 - val_accuracy: 0.1423 - 935ms/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "97/97 - 1s - loss: 2.7715 - accuracy: 0.1509 - val_loss: 2.7397 - val_accuracy: 0.1514 - 1s/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "97/97 - 1s - loss: 2.7654 - accuracy: 0.1512 - val_loss: 2.7383 - val_accuracy: 0.1539 - 928ms/epoch - 10ms/step\n",
            "Epoch 17/100\n",
            "97/97 - 1s - loss: 2.7585 - accuracy: 0.1499 - val_loss: 2.7393 - val_accuracy: 0.1475 - 928ms/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "97/97 - 1s - loss: 2.7498 - accuracy: 0.1613 - val_loss: 2.7286 - val_accuracy: 0.1565 - 955ms/epoch - 10ms/step\n",
            "Epoch 19/100\n",
            "97/97 - 1s - loss: 2.7444 - accuracy: 0.1619 - val_loss: 2.7317 - val_accuracy: 0.1462 - 1s/epoch - 11ms/step\n",
            "Epoch 20/100\n",
            "97/97 - 2s - loss: 2.7392 - accuracy: 0.1532 - val_loss: 2.7264 - val_accuracy: 0.1514 - 2s/epoch - 16ms/step\n",
            "Epoch 21/100\n",
            "97/97 - 1s - loss: 2.7254 - accuracy: 0.1645 - val_loss: 2.7178 - val_accuracy: 0.1514 - 1s/epoch - 15ms/step\n",
            "Epoch 22/100\n",
            "97/97 - 1s - loss: 2.7217 - accuracy: 0.1619 - val_loss: 2.7135 - val_accuracy: 0.1695 - 1s/epoch - 10ms/step\n",
            "Epoch 23/100\n",
            "97/97 - 1s - loss: 2.7115 - accuracy: 0.1600 - val_loss: 2.7023 - val_accuracy: 0.1708 - 976ms/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "97/97 - 1s - loss: 2.7016 - accuracy: 0.1639 - val_loss: 2.6964 - val_accuracy: 0.1682 - 961ms/epoch - 10ms/step\n",
            "Epoch 25/100\n",
            "97/97 - 1s - loss: 2.6955 - accuracy: 0.1694 - val_loss: 2.6887 - val_accuracy: 0.1565 - 953ms/epoch - 10ms/step\n",
            "Epoch 26/100\n",
            "97/97 - 1s - loss: 2.6841 - accuracy: 0.1668 - val_loss: 2.6970 - val_accuracy: 0.1617 - 1s/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "97/97 - 1s - loss: 2.6768 - accuracy: 0.1681 - val_loss: 2.6817 - val_accuracy: 0.1824 - 1s/epoch - 11ms/step\n",
            "Epoch 28/100\n",
            "97/97 - 1s - loss: 2.6687 - accuracy: 0.1697 - val_loss: 2.6838 - val_accuracy: 0.1837 - 985ms/epoch - 10ms/step\n",
            "Epoch 29/100\n",
            "97/97 - 1s - loss: 2.6616 - accuracy: 0.1648 - val_loss: 2.6672 - val_accuracy: 0.1915 - 954ms/epoch - 10ms/step\n",
            "Epoch 30/100\n",
            "97/97 - 1s - loss: 2.6529 - accuracy: 0.1736 - val_loss: 2.6763 - val_accuracy: 0.1746 - 1s/epoch - 11ms/step\n",
            "Epoch 31/100\n",
            "97/97 - 1s - loss: 2.6458 - accuracy: 0.1826 - val_loss: 2.6713 - val_accuracy: 0.1876 - 1s/epoch - 11ms/step\n",
            "Epoch 32/100\n",
            "97/97 - 2s - loss: 2.6394 - accuracy: 0.1742 - val_loss: 2.6720 - val_accuracy: 0.1889 - 2s/epoch - 18ms/step\n",
            "Epoch 33/100\n",
            "97/97 - 1s - loss: 2.6306 - accuracy: 0.1752 - val_loss: 2.6623 - val_accuracy: 0.1746 - 1s/epoch - 14ms/step\n",
            "Epoch 34/100\n",
            "97/97 - 1s - loss: 2.6243 - accuracy: 0.1839 - val_loss: 2.6572 - val_accuracy: 0.1966 - 946ms/epoch - 10ms/step\n",
            "Epoch 35/100\n",
            "97/97 - 1s - loss: 2.6184 - accuracy: 0.1817 - val_loss: 2.6609 - val_accuracy: 0.1992 - 933ms/epoch - 10ms/step\n",
            "Epoch 36/100\n",
            "97/97 - 1s - loss: 2.6155 - accuracy: 0.1836 - val_loss: 2.6641 - val_accuracy: 0.1746 - 1s/epoch - 11ms/step\n",
            "Epoch 37/100\n",
            "97/97 - 1s - loss: 2.6080 - accuracy: 0.1885 - val_loss: 2.6569 - val_accuracy: 0.1902 - 990ms/epoch - 10ms/step\n",
            "Epoch 38/100\n",
            "97/97 - 1s - loss: 2.6011 - accuracy: 0.1898 - val_loss: 2.6567 - val_accuracy: 0.2005 - 952ms/epoch - 10ms/step\n",
            "Epoch 39/100\n",
            "97/97 - 1s - loss: 2.5983 - accuracy: 0.1859 - val_loss: 2.6562 - val_accuracy: 0.1902 - 997ms/epoch - 10ms/step\n",
            "Epoch 40/100\n",
            "97/97 - 1s - loss: 2.5953 - accuracy: 0.1904 - val_loss: 2.6538 - val_accuracy: 0.1876 - 1s/epoch - 11ms/step\n",
            "Epoch 41/100\n",
            "97/97 - 1s - loss: 2.5895 - accuracy: 0.1891 - val_loss: 2.6606 - val_accuracy: 0.1992 - 1s/epoch - 12ms/step\n",
            "Epoch 42/100\n",
            "97/97 - 1s - loss: 2.5846 - accuracy: 0.1940 - val_loss: 2.6550 - val_accuracy: 0.1940 - 1s/epoch - 11ms/step\n",
            "Epoch 43/100\n",
            "97/97 - 2s - loss: 2.5813 - accuracy: 0.1943 - val_loss: 2.6715 - val_accuracy: 0.1966 - 2s/epoch - 17ms/step\n",
            "Epoch 44/100\n",
            "97/97 - 1s - loss: 2.5750 - accuracy: 0.1982 - val_loss: 2.6541 - val_accuracy: 0.1915 - 1s/epoch - 14ms/step\n",
            "Epoch 45/100\n",
            "97/97 - 1s - loss: 2.5715 - accuracy: 0.1914 - val_loss: 2.6588 - val_accuracy: 0.1940 - 989ms/epoch - 10ms/step\n",
            "Epoch 46/100\n",
            "97/97 - 1s - loss: 2.5651 - accuracy: 0.2021 - val_loss: 2.6632 - val_accuracy: 0.1863 - 1s/epoch - 10ms/step\n",
            "Epoch 47/100\n",
            "97/97 - 1s - loss: 2.5614 - accuracy: 0.1985 - val_loss: 2.6616 - val_accuracy: 0.1695 - 960ms/epoch - 10ms/step\n",
            "Epoch 48/100\n",
            "97/97 - 1s - loss: 2.5583 - accuracy: 0.1972 - val_loss: 2.6537 - val_accuracy: 0.1889 - 1s/epoch - 10ms/step\n",
            "Epoch 49/100\n",
            "97/97 - 1s - loss: 2.5542 - accuracy: 0.2011 - val_loss: 2.6655 - val_accuracy: 0.1721 - 1s/epoch - 11ms/step\n",
            "Epoch 50/100\n",
            "97/97 - 1s - loss: 2.5467 - accuracy: 0.2005 - val_loss: 2.6528 - val_accuracy: 0.1824 - 1s/epoch - 11ms/step\n",
            "Epoch 51/100\n",
            "97/97 - 1s - loss: 2.5480 - accuracy: 0.2073 - val_loss: 2.6572 - val_accuracy: 0.1902 - 938ms/epoch - 10ms/step\n",
            "Epoch 52/100\n",
            "97/97 - 1s - loss: 2.5419 - accuracy: 0.2053 - val_loss: 2.6749 - val_accuracy: 0.1850 - 1s/epoch - 11ms/step\n",
            "Epoch 53/100\n",
            "97/97 - 1s - loss: 2.5393 - accuracy: 0.2073 - val_loss: 2.6773 - val_accuracy: 0.1889 - 1s/epoch - 11ms/step\n",
            "Epoch 54/100\n",
            "97/97 - 1s - loss: 2.5326 - accuracy: 0.2089 - val_loss: 2.6710 - val_accuracy: 0.1889 - 1s/epoch - 13ms/step\n",
            "Epoch 55/100\n",
            "97/97 - 2s - loss: 2.5281 - accuracy: 0.2121 - val_loss: 2.6762 - val_accuracy: 0.1876 - 2s/epoch - 16ms/step\n",
            "Epoch 56/100\n",
            "97/97 - 1s - loss: 2.5240 - accuracy: 0.2179 - val_loss: 2.6757 - val_accuracy: 0.1669 - 1s/epoch - 10ms/step\n",
            "Epoch 57/100\n",
            "97/97 - 1s - loss: 2.5196 - accuracy: 0.2118 - val_loss: 2.6663 - val_accuracy: 0.1811 - 931ms/epoch - 10ms/step\n",
            "Epoch 58/100\n",
            "97/97 - 1s - loss: 2.5168 - accuracy: 0.2131 - val_loss: 2.6786 - val_accuracy: 0.1863 - 964ms/epoch - 10ms/step\n",
            "Epoch 59/100\n",
            "97/97 - 1s - loss: 2.5163 - accuracy: 0.2095 - val_loss: 2.6774 - val_accuracy: 0.1850 - 1s/epoch - 11ms/step\n",
            "Epoch 60/100\n",
            "97/97 - 1s - loss: 2.5059 - accuracy: 0.2189 - val_loss: 2.6766 - val_accuracy: 0.1811 - 1s/epoch - 11ms/step\n",
            "Epoch 61/100\n",
            "97/97 - 1s - loss: 2.5035 - accuracy: 0.2131 - val_loss: 2.6797 - val_accuracy: 0.1785 - 944ms/epoch - 10ms/step\n",
            "Epoch 62/100\n",
            "97/97 - 1s - loss: 2.4984 - accuracy: 0.2189 - val_loss: 2.6790 - val_accuracy: 0.1889 - 989ms/epoch - 10ms/step\n",
            "Epoch 63/100\n",
            "97/97 - 1s - loss: 2.4972 - accuracy: 0.2199 - val_loss: 2.6818 - val_accuracy: 0.1902 - 941ms/epoch - 10ms/step\n",
            "Epoch 64/100\n",
            "97/97 - 1s - loss: 2.4894 - accuracy: 0.2186 - val_loss: 2.6851 - val_accuracy: 0.1902 - 1s/epoch - 11ms/step\n",
            "Epoch 65/100\n",
            "97/97 - 1s - loss: 2.4866 - accuracy: 0.2270 - val_loss: 2.6785 - val_accuracy: 0.1863 - 1s/epoch - 11ms/step\n",
            "Epoch 66/100\n",
            "97/97 - 1s - loss: 2.4806 - accuracy: 0.2341 - val_loss: 2.6941 - val_accuracy: 0.1850 - 1s/epoch - 14ms/step\n",
            "Epoch 67/100\n",
            "97/97 - 1s - loss: 2.4752 - accuracy: 0.2280 - val_loss: 2.6917 - val_accuracy: 0.1772 - 1s/epoch - 15ms/step\n",
            "Epoch 68/100\n",
            "97/97 - 1s - loss: 2.4741 - accuracy: 0.2286 - val_loss: 2.6941 - val_accuracy: 0.1617 - 948ms/epoch - 10ms/step\n",
            "Epoch 69/100\n",
            "97/97 - 1s - loss: 2.4722 - accuracy: 0.2205 - val_loss: 2.6902 - val_accuracy: 0.1928 - 940ms/epoch - 10ms/step\n",
            "Epoch 70/100\n",
            "97/97 - 1s - loss: 2.4668 - accuracy: 0.2322 - val_loss: 2.6884 - val_accuracy: 0.1876 - 991ms/epoch - 10ms/step\n",
            "Epoch 71/100\n",
            "97/97 - 1s - loss: 2.4600 - accuracy: 0.2338 - val_loss: 2.6962 - val_accuracy: 0.1824 - 1s/epoch - 11ms/step\n",
            "Epoch 72/100\n",
            "97/97 - 1s - loss: 2.4549 - accuracy: 0.2396 - val_loss: 2.7060 - val_accuracy: 0.1772 - 966ms/epoch - 10ms/step\n",
            "Epoch 73/100\n",
            "97/97 - 1s - loss: 2.4523 - accuracy: 0.2387 - val_loss: 2.6993 - val_accuracy: 0.1876 - 934ms/epoch - 10ms/step\n",
            "Epoch 74/100\n",
            "97/97 - 1s - loss: 2.4495 - accuracy: 0.2361 - val_loss: 2.7002 - val_accuracy: 0.1940 - 948ms/epoch - 10ms/step\n",
            "Epoch 75/100\n",
            "97/97 - 1s - loss: 2.4450 - accuracy: 0.2361 - val_loss: 2.6998 - val_accuracy: 0.1992 - 957ms/epoch - 10ms/step\n",
            "Epoch 76/100\n",
            "97/97 - 1s - loss: 2.4375 - accuracy: 0.2426 - val_loss: 2.7065 - val_accuracy: 0.1902 - 927ms/epoch - 10ms/step\n",
            "Epoch 77/100\n",
            "97/97 - 1s - loss: 2.4356 - accuracy: 0.2383 - val_loss: 2.7101 - val_accuracy: 0.1953 - 1s/epoch - 11ms/step\n",
            "Epoch 78/100\n",
            "97/97 - 2s - loss: 2.4284 - accuracy: 0.2432 - val_loss: 2.7039 - val_accuracy: 0.1876 - 2s/epoch - 17ms/step\n",
            "Epoch 79/100\n",
            "97/97 - 1s - loss: 2.4252 - accuracy: 0.2474 - val_loss: 2.7090 - val_accuracy: 0.1953 - 1s/epoch - 15ms/step\n",
            "Epoch 80/100\n",
            "97/97 - 1s - loss: 2.4195 - accuracy: 0.2477 - val_loss: 2.7131 - val_accuracy: 0.1902 - 984ms/epoch - 10ms/step\n",
            "Epoch 81/100\n",
            "97/97 - 1s - loss: 2.4176 - accuracy: 0.2448 - val_loss: 2.7233 - val_accuracy: 0.1992 - 978ms/epoch - 10ms/step\n",
            "Epoch 82/100\n",
            "97/97 - 1s - loss: 2.4134 - accuracy: 0.2500 - val_loss: 2.7215 - val_accuracy: 0.2005 - 1s/epoch - 10ms/step\n",
            "Epoch 83/100\n",
            "97/97 - 1s - loss: 2.4073 - accuracy: 0.2568 - val_loss: 2.7209 - val_accuracy: 0.1798 - 940ms/epoch - 10ms/step\n",
            "Epoch 84/100\n",
            "97/97 - 1s - loss: 2.4021 - accuracy: 0.2549 - val_loss: 2.7174 - val_accuracy: 0.1643 - 951ms/epoch - 10ms/step\n",
            "Epoch 85/100\n",
            "97/97 - 1s - loss: 2.3991 - accuracy: 0.2497 - val_loss: 2.7249 - val_accuracy: 0.1966 - 959ms/epoch - 10ms/step\n",
            "Epoch 86/100\n",
            "97/97 - 1s - loss: 2.3940 - accuracy: 0.2578 - val_loss: 2.7314 - val_accuracy: 0.1824 - 1s/epoch - 11ms/step\n",
            "Epoch 87/100\n",
            "97/97 - 1s - loss: 2.3935 - accuracy: 0.2494 - val_loss: 2.7278 - val_accuracy: 0.1966 - 1s/epoch - 11ms/step\n",
            "Epoch 88/100\n",
            "97/97 - 1s - loss: 2.3897 - accuracy: 0.2435 - val_loss: 2.7322 - val_accuracy: 0.1863 - 991ms/epoch - 10ms/step\n",
            "Epoch 89/100\n",
            "97/97 - 1s - loss: 2.3843 - accuracy: 0.2584 - val_loss: 2.7346 - val_accuracy: 0.1824 - 1s/epoch - 12ms/step\n",
            "Epoch 90/100\n",
            "97/97 - 2s - loss: 2.3796 - accuracy: 0.2532 - val_loss: 2.7333 - val_accuracy: 0.1889 - 2s/epoch - 17ms/step\n",
            "Epoch 91/100\n",
            "97/97 - 1s - loss: 2.3742 - accuracy: 0.2529 - val_loss: 2.7374 - val_accuracy: 0.1940 - 1s/epoch - 11ms/step\n",
            "Epoch 92/100\n",
            "97/97 - 1s - loss: 2.3688 - accuracy: 0.2584 - val_loss: 2.7353 - val_accuracy: 0.1876 - 1s/epoch - 10ms/step\n",
            "Epoch 93/100\n",
            "97/97 - 1s - loss: 2.3738 - accuracy: 0.2600 - val_loss: 2.7440 - val_accuracy: 0.1902 - 976ms/epoch - 10ms/step\n",
            "Epoch 94/100\n",
            "97/97 - 1s - loss: 2.3614 - accuracy: 0.2646 - val_loss: 2.7381 - val_accuracy: 0.1837 - 960ms/epoch - 10ms/step\n",
            "Epoch 95/100\n",
            "97/97 - 1s - loss: 2.3614 - accuracy: 0.2655 - val_loss: 2.7379 - val_accuracy: 0.1979 - 982ms/epoch - 10ms/step\n",
            "Epoch 96/100\n",
            "97/97 - 1s - loss: 2.3566 - accuracy: 0.2685 - val_loss: 2.7475 - val_accuracy: 0.1682 - 1s/epoch - 10ms/step\n",
            "Epoch 97/100\n",
            "97/97 - 1s - loss: 2.3549 - accuracy: 0.2655 - val_loss: 2.7400 - val_accuracy: 0.1889 - 1s/epoch - 11ms/step\n",
            "Epoch 98/100\n",
            "97/97 - 1s - loss: 2.3505 - accuracy: 0.2665 - val_loss: 2.7380 - val_accuracy: 0.1863 - 990ms/epoch - 10ms/step\n",
            "Epoch 99/100\n",
            "97/97 - 1s - loss: 2.3479 - accuracy: 0.2698 - val_loss: 2.7489 - val_accuracy: 0.1928 - 1s/epoch - 10ms/step\n",
            "Epoch 100/100\n",
            "97/97 - 1s - loss: 2.3417 - accuracy: 0.2672 - val_loss: 2.7430 - val_accuracy: 0.1966 - 933ms/epoch - 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(model, mapping, input_word):\n",
        "  encoded_input = [mapping.get(char, 0) for char in input_word]\n",
        "  encoded_input = pad_sequences([encoded_input], maxlen= x.shape[1], truncating = 'pre') / float(len(mapping))\n",
        "  encoded_input = encoded_input.reshape(encoded_input.shape[0],encoded_input.shape[1], 1)\n",
        "  prediction = model.predict(encoded_input, verbose=0)\n",
        "  index = np.argmax(prediction)\n",
        "  out_word = ''\n",
        "  for char, idx in mapping.items():\n",
        "    if idx == index:\n",
        "      out_word += char\n",
        "      break\n",
        "  return out_word"
      ],
      "metadata": {
        "id": "S3lFmGLSK4DF"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct_spelling(model, mapping, 'Yevoze'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrM_ueYLObXl",
        "outputId": "8a27d52d-1f06-4f35-996d-9760eaf2c637"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling_beam_search(model, mapping, input_word, k=3, sequence_max_len=100):\n",
        "    input_sequence = [mapping.get(char, 0) for char in input_word]\n",
        "    input_sequence = pad_sequences([input_sequence], maxlen=sequence_max_len, truncating='pre') / float(len(mapping))\n",
        "    input_sequence = input_sequence.reshape(input_sequence.shape[0], input_sequence.shape[1], 1)\n",
        "\n",
        "    # Initialize beam search\n",
        "    sequences = [[list(), 0.0]]\n",
        "\n",
        "    # Loop over sequence length\n",
        "    for _ in range(sequence_max_len):\n",
        "        all_candidates = list()\n",
        "        # Loop over each sequence in the beam\n",
        "        for i in range(len(sequences)):\n",
        "            seq, score = sequences[i]\n",
        "            yhat = model.predict([seq])[0]\n",
        "            # Loop over each possible next token\n",
        "            for j in range(len(yhat)):\n",
        "                # Create a new candidate sequence\n",
        "                candidate = [seq + [j], score - np.log(yhat[j])]\n",
        "                all_candidates.append(candidate)\n",
        "        # Rank all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        # Select k best\n",
        "        sequences = ordered[:k]\n",
        "    best_sequence = sequences[0]\n",
        "    out_word = [x for x in best_sequence[0]]\n",
        "    return ''.join(out_word)\n",
        "\n",
        "# Testing the prediction\n",
        "print(correct_spelling_beam_search(model, mapping, 'Yevozen'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "bXtRcVeHRA8s",
        "outputId": "87aff990-7685-48d4-fd5a-c43bc382fe01"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-43b2e613b19f>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Testing the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_spelling_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Yevozen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-43b2e613b19f>\u001b[0m in \u001b[0;36mcorrect_spelling_beam_search\u001b[0;34m(model, mapping, input_word, k, sequence_max_len)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Loop over each possible next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         num_samples = set(\n\u001b[1;32m    259\u001b[0m             \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         ).pop()\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'pop from an empty set'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def char_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "\n",
        "def char_vector(text, n, vocabulary):\n",
        "    text_ngrams = char_ngrams(text, n)\n",
        "    return np.array([text_ngrams.count(i) for i in vocabulary])\n",
        "\n",
        "def cosine_similarity_char_ngram(text1, text2, n):\n",
        "    ngrams_text1 = char_ngrams(text1, n)\n",
        "    ngrams_text2 = char_ngrams(text2, n)\n",
        "    vocabulary = list(set(ngrams_text1 + ngrams_text2))\n",
        "    vector1 = char_vector(text1, n, vocabulary)\n",
        "    vector2 = char_vector(text2, n, vocabulary)\n",
        "    if norm(vector1) != 0 and norm(vector2) != 0:\n",
        "        cos_sim = dot(vector1, vector2)/(norm(vector1)*norm(vector2))\n",
        "    else:\n",
        "        cos_sim = 0\n",
        "    return cos_sim\n",
        "\n",
        "\n",
        "def correct_spelling_cosine(word, drug_dict, ngram=2):\n",
        "    similarities = [(cosine_similarity_char_ngram(word, key, ngram), key) for key in drug_dict.keys()]\n",
        "    similarities.sort(reverse=True)\n",
        "    return [similarities[i][1] for i in range(0,len(similarities))]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5oEBBi98U4ZT"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct_spelling_cosine('pekadin', drug_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dambNxPVVBai",
        "outputId": "4712035c-3a14-435b-a299-7b249d773a02"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dekadin', 'dikacin', 'yacadine', 'dinosin', 'yinka', 'yekin', 'dilkab', 'diepin', 'yukacin', 'divaine', 'dicline', 'yoridine', 'domtidin', 'dizapear', 'dicolead', 'diapyrin', 'yukacin-o', 'dil', 'davaindia', 'yzin', 'yaka', 'digo', 'dice', 'additionally,', 'yzine', 'yucin', 'yosin', 'yline', 'yezdi', 'dutin', 'dupin', 'dppin', 'dmine', 'divon', 'dival', 'ditim', 'disal', 'diroz', 'diran', 'dipod', 'difen', 'derek', 'depel', 'yoxtin', 'yoflin', 'ycline', 'yasmin', 'yamini', 'yamika', 'yadrox', 'dxfine', 'duowin', 'duopep', 'divsys', 'divcet', 'dithro', 'disera', 'dipkem', 'diopan', 'diolof', 'dioflo', 'dilzem', 'diflox', 'diflam', 'dicnos', 'dicmol', 'dicmak', 'diclom', 'deswin', 'dellin', 'yoritad', 'yorimin', 'yoridiv', 'yevomin', 'yetoxin', 'dynocad', 'drozine', 'dpazine', 'douxper', 'docitin', 'dmentin', 'dixovan', 'divorel', 'divazen', 'divasag', 'divacof', 'dispasm', 'disoval', 'dilocus', 'dilnorm', 'difsoon', 'difisal', 'difenic', 'diecold', 'dicotan', 'diclove', 'dicipam', 'dicerex', 'dibto-t', 'diarlop', 'desipep', 'deripen', 'dapexia', 'damylin', 'yuvakast', 'yoxyzine', 'yonikast', 'yocofine', 'yitolinz', 'yezdi-mr', 'yespenta', 'yesgalin', 'yaniline', 'dupent-d', 'domperal', 'dolokind', 'divalpro', 'divalcot', 'divaa-od', 'ditrazol', 'diprosal', 'diplomax', 'diplocef', 'diphecol', 'diltoris', 'dilophen', 'diclozor', 'diclotal', 'diclotaj', 'diclosun', 'diclonij', 'diclomed', 'dicloliv', 'diclofit', 'diapride', 'delpizin', 'deflotek', 'deflapin', 'yorinor', 'yogamin-m', 'yaninim', 'doxonin-m', 'divatrend', 'divaltime', 'diplifast', 'dioglip-h', 'digpod-iv', 'diclosung', 'diclosaid', 'dicloross', 'diclopace', 'dicloneed', 'diclom-sp', 'dicloflam', 'dianorm-m', 'diakon-mr', 'diabetrol', 'deflachek', 'yodilite-l', 'yetoxin-oz', 'doximin-lb', 'douxper-sb', 'diplena-sp', 'diltigesic', 'diclosab-p', 'dicloforte', 'diaten-max', 'dermotriad', 'divalprowyn', 'difarnac-mr', 'diclozor-sp', 'diclozit-sp', 'diclonex-mr', 'daad-raaath', 'yuxa', 'yuvti', 'yuvpan', 'yuvimont-fx', 'yuvimont', 'yuvikuff', 'yuvigra', 'yuvigest', 'yuviflox-oz', 'yuviflox', 'yuvidox-cv', 'yuvidox', 'yuvicold', 'yuviclav', 'yuvicef-x', 'yuvicef-o', 'yuvicef', 'yuvican', 'yuval', 'yuvagra', 'yutopar', 'yutolan', 'yupan', 'yumox', 'yumipod', 'yulose', 'yuliprist', 'yulid', 'yugprol', 'yugclave', 'yufexim', 'yudolic', 'yudlox', 'yuclox', 'ytral', 'ysoflox', 'ysetxim', 'ysacef', 'yrof', 'yrab-d', 'yrab', 'ypodox', 'ypan', 'yozole', 'yozith', 'yozid', 'yoyocold', 'yoyo', 'yoxy', 'yourxim', 'yourflox', 'yourclox', 'yourcef', 'yourab', 'your', 'youpod', 'youpan', 'yougaba', 'you', 'yotel', 'yorul', 'yoromol', 'yormet', 'yorkmide', 'yorker', 'yorkee', 'yorizole', 'yorirab', 'yoripan', 'yorion', 'yorimox', 'yorimeth', 'yoriflox', 'yorifex', 'yoridol', 'yoriclav', 'yordom', 'yorcet', 'yorab', 'yopreg', 'yopon', 'yoor', 'yoodh', 'yonril', 'yonireb', 'yonimac', 'yonim', 'yonice', 'yonflox', 'yoncef', 'yomzole-d', 'yomzole', 'yomfate-o', 'yomfate', 'yom', 'yolaz', 'yoketo', 'yojus', 'yohit', 'yogazam', 'yogatel-am', 'yogatel-ah', 'yogatel', 'yogaros', 'yogaprol', 'yogamet-trio', 'yogamet-gmp', 'yogamet-gm', 'yogabos', 'yoflox', 'yofix', 'yodoxim', 'yocozol', 'yocoset', 'yocoliv', 'yocoflu', 'yocofate', 'yococip', 'yococef', 'yoclav', 'yocet', 'yocef', 'yocam', 'ymetlo', 'ymet', 'ykuf', 'yivan-sp', 'yivan-p', 'yivan', 'yflox', 'yf', 'yezole', 'yexitil', 'yewloc', 'yewflo', 'yewcet', 'yewcan', 'yevozith', 'yevozen-o', 'yevozen-cv', 'yevozen', 'yevorab-d', 'yevorab', 'yevonem', 'yevonate', 'yevon', 'yevolid', 'yevofung', 'yevof-or', 'yevof', 'yevoclav', 'yevoc', 'yevo', 'yetzone', 'yetro', 'yetomox', 'yetofol', 'yetoflo', 'yetmol', 'yetbrox', 'yetacef', 'yestop', 'yestilo', 'yestic', 'yesthox', 'yester', 'yesrox', 'yespram', 'yespod', 'yesoppi', 'yesnec-sp', 'yesmox', 'yesglyz', 'yesglim', 'yesflu', 'yesfexo', 'yescort', 'yescef', 'yescam', 'yesbat', 'yes', 'yerab', 'yera', 'yepred', 'yeom', 'yenti', 'yenta', 'yenocort', 'yencox-mr', 'yencox', 'yenate', 'yenac', 'yemetil', 'yelocip', 'yelocet', 'yellowpam', 'yelfex', 'yelcef-o', 'yelcef-d', 'yelcef-clav', 'yelcef', 'yefur', 'yefix-lb', 'yefix', 'yees-d', 'yees', 'yecold', 'yeclo', 'yearcet', 'yeale', 'ydro', 'ycomik', 'yclo', 'ycim', 'ycer-th', 'ycer-sp', 'ycer-p', 'ycer', 'ycepo', 'ybz', 'yazone', 'yazi', 'yaz', 'yaxoslide', 'yaxolizer', 'yaxoflex', 'yaxocort', 'yaxocip', 'yaxocet', 'yaxi', 'yax', 'yaw', 'yavisc', 'yavir', 'yasofyl', 'yashdon-dsr', 'yashdon', 'yashcold', 'yash', 'yaracoxia', 'yapat', 'yanitel', 'yanipreg', 'yanifix', 'yanicold', 'yaniclox', 'yanic', 'yanci', 'yana', 'yamnac-t', 'yamnac-sp', 'yamnac-p', 'yamnac-mr', 'yamipride', 'yamiclav', 'yama', 'yalithim', 'yakrit', 'yacert', 'yaccalyte', 'yacca-lyte', 'yacca', 'yacaphen', 'yacanor', 'yabeta', 'yab', 'y-fi', 'y-doxime-cv', 'y-doxime', 'y-clast', 'y', 'to', 'this', 'take', 'it', 'give', 'follow', 'dzid', 'dz', 'dyno', 'dynazone-sp', 'dynatroy', 'dynaford', 'dycnac', 'dyclo-aq', 'dval', 'duvet-cv', 'dutron', 'dutas', 'duram-c', 'duralyn', 'duphaston', 'duobrim', 'duluta', 'duloxyl', 'duloxefit', 'dulox', 'dulodep', 'ducoxib', 'ducati', 'dubnil', 'dub', 'duaxit', 'dtori', 'dto', 'dtmol', 'dtm', 'drzeth', 'drycef', 'druzem', 'drunim', 'dru-sp', 'drtglip', 'droxytop', 'droxlect', 'droxib', 'droway', 'drove-mf', 'drotway', 'drotel', 'drotayew', 'drose', 'dropar', 'dromred', 'dromit', 'drogex-m', 'drofi', 'droact', 'drmet', 'dreza', 'drexel', 'dremz', 'dragon', 'dracef', 'dr', 'dpano', 'dp1', 'doxyrec', 'doxynor', 'doxynec', 'doxymont', 'doxygee', 'doxycon-lb', 'doxy', 'doxvit', 'doxtil', 'doxsom', 'doxoript', 'doxopic', 'doxone', 'doxoll-ml', 'doxoll', 'doxipro', 'doxion', 'doximon', 'doximed', 'doxigru', 'doxid', 'doxe', 'doxapress', 'dox', 'doupod-cl', 'doucef', 'dotpun', 'dotcam', 'dotam', 'doris', 'dorapil', 'dopsy', 'donrid', 'donispas', 'domvent', 'domq', 'domicid', 'domgerd', 'domfen-p', 'domez', 'dom', 'dolycox-t', 'dolser', 'dolozox', 'dolotram', 'dolonex', 'dolomas', 'doloflex-mr', 'doloby', 'dolfre', 'dolcuf-ls', 'dolan', 'dol', 'dokim', 'dofast', 'dofasma', 'dofaq', 'doevil', 'dodo-mr', 'docmef', 'dobesil-h', 'do', 'dmfix-cv', 'dkoff', 'dhista', 'dgpan', 'dfriz', 'dflam', 'dezoliv', 'dezithro', 'dezar', 'dezacor', 'dexzy', 'dextum', 'dexoren', 'deximon-px', 'dexamethasone', 'dexagee', 'dexacore', 'dex-pc', 'deverab', 'devas', 'desval', 'desonex', 'desol', 'dermovent', 'dermitch-c', 'dermipred', 'dermavir', 'derazo', 'depther', 'deproz', 'deprick', 'depracort', 'deploc', 'deplam', 'depitame', 'depicor', 'depcure', 'denim', 'den-la', 'den', 'demo', 'demef', 'delpride', 'delpomont', 'delicita', 'delfidase-plus', 'delacort', 'defzorix', 'defzo', 'defwell', 'defpalm', 'defozone', 'defol', 'deflasil', 'deflaris', 'deflacrom', 'defenz', 'defalen', 'defagood', 'deemo', 'decotram', 'decor', 'declan-s', 'decitas', 'decet-m', 'decef', 'decamox', 'dec', 'debistal', 'debi', 'dcefu', 'dcef-o', 'daylevo', 'daxicort', 'davlone', 'davflox', 'davfix', 'davbet', 'datrix', 'dase', 'dartglip', 'darmavir', 'darbitop', 'daptoma', 'dapanova', 'dapakedni', 'danzol-b', 'dansy', 'daneeb', 'danavish', 'damol', 'dalus', 'dalfirate', 'dalcihit', 'dalbon-sp', 'dalbon', 'dakpan', 'dairo', 'dailyglim', 'dafcor', 'dacares', 'da', 'd-veniz', 'd-vel', 'd-glip', 'd-clob', 'd']\n"
          ]
        }
      ]
    }
  ]
}